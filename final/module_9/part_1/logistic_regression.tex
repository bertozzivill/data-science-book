\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{}
    \pretitle{\vspace{\droptitle}}
  \posttitle{}
    \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  

\begin{document}

\section*{(PART) Logistic Regression and Machine
Learning}\label{part-logistic-regression-and-machine-learning}
\addcontentsline{toc}{section}{(PART) Logistic Regression and Machine
Learning}

\section{Logistic Regression}\label{logistic-regression}

\textbf{todo: modify this to go before any ml lingo, check how this
lines up with the lecture in canvas} Logistic regression straddles the
boundary between machine learning and statistics-- it's a classification
algorithm, but it uses (statistical) regression techniques. Logistic
regressions are easy to interpret if you're comfortable with natural
logarithms, exponentiation, and odds-- let's review these topic before
we get into the details of how logistic regression works.

\subsection{Script for e/log
definitions}\label{script-for-elog-definitions}

Definitions: e, exponents, logs

\subsubsection{e}\label{e}

Let's start by defining the number e.

\[ e = 2.7182818284590452353602874713527... \]

In mathematics, ``e'' doesn't represent a letter, it represents a
number. Like pi, e is an irrational number-- that is, its decimal
representation goes on forever with no repetitions or other patterns--
and also like pi, e is extremely important both in pure and applied
mathematics.

\subsubsection{Exponents}\label{exponents}

You may already be familiar with the idea of exponentiation, or taking a
number ``to the power'' of another number.

\[ b^n \]

Exponentiation requires two values: the ``base'' value b, and the
``exponent'' n. This operation means ``multiply \(b\) by itself \(n\)
times''.

So, if our base is 10 and our exponent is 2, we multiply 10 by itself
twice:

\[ 10^2 = 10*10 = 100 \] \textbf{todo: more clearly delineate this into
the beginning/end of a rules list} You can have negative exponents,
which is the same as dividing by a positive exponent:

\[ 10^{-2}=\frac{1}{10^2} = \frac{1}{100} = 0.01 \]

And, importantly, that you can have non-integer exponents, which it's
easiest to calculate using a computer:

\textbf{todo: break this down}

\[ 10^{1.5} = 31.62278\]

The base value, b, can also be whatever you want it to. it's very common
to have b take the value of e:

\[ e^x \]

For any exponentiated value b, b\^{}1 is always b and b\^{}0 is always
1:

\[ b^1=b \\b^0 = 1 \]

Exponentiated values have the nice property that addition in the
exponents can also be expressed as multiplication in the bases:

\[ e^{a + b} = e^a * e^b \]

Here's a concrete example of this:

\[ 10^{2+3} = 10^5 =  100,000 \\ 10^{2+3} = 10^2 * 10^3 = 100 * 1000 = 100,000 \]

\subsubsection{Logarithms}\label{logarithms}

\paragraph{Inverse function}\label{inverse-function}

An \emph{inverse function} is something that does the opposite of some
other function. Let's say we have a funtion f(x) that takes a value x
and multiplies it by 2:

\[ f(x) = 2x \]

The \emph{inverse function} of f(x) would be a function that
\emph{divides} a value by 2:

\[ g(x) = x/2 \]

You know that two functions are the inverse of each other when
performing them in sequence recovers the original x value. For example,
let's say our x value is 4. Let's run f(x) on this value, \emph{then}
run g(x) on the output of that function:

\[ f(4) = 2*4 = 8 \\ g(8) = 8/2 = 4\]

you can also see this by nesting the two functions inside each other:

\[ g(f(x)) = \frac{f(x)}{2} = \frac{2x}{2} = x \]

\paragraph{Logarithms}\label{logarithms-1}

In the same way that x/2 is the inverse function of 2x, a
\emph{logarithm} is the inverse function to exponentiation. Like
exponentiation, logarithms have a \emph{base}, and only exponential
functions and logarithmic functions with the \emph{same base} are
inverses of each other. So:

\[ log_{10}(x) \]

and

\[ 10^x \]

Are inverse functions to each other, but

\[ log_{10}(x) \]

and

\[ e^x \] are \emph{not} inverse functions to each other.

When you run a logarithmic function with a certain base b on a number x,
it returns \emph{the number you would need to exponentiate b by in order
to get x}. For example, log base 10 of 100 is 2:

\[ log_{10}(100) = 2\]

What this means is: ``You have to raise 10 to the power of 2 in order to
get 100''

You can see from this example how the two functions are inverse to each
other:

\[ log_{10}(10^2) = log_{10}(100) = 2 \]

The function e\^{}x is so common that its inverse function log base e is
called the ``natural logarithm''-- it's often written shorthand as
simply ln(x).

\[ e^x \]

\[log_e(x) \\ ln(x)\]

Remember:

\[ ln(e^x) = x \]

\subsection{Odds}\label{odds}

Now, let's briefly define some terms associated with probability:
\emph{odds} and \emph{odds ratio}

\textbf{todo: start with probability reminder before moving to odds}

A single \emph{odds} is itself a ratio: the ratio of how likely a thing
is to happen, compared to it not happening. If there are ``3 to 1 odds''
of something happening, it means that it will happen 3 out of 4 times,
or that the probability of it happening is 75\%. If something has a
probability p of happening, the odds are defined as:

\[ odds = \frac{p}{1-p} \] If something has a 25\% chance of happening,
the odds are :

\[ odds = \frac{0.25}{1-0.25} = \frac{0.25}{0.75} = 1/3 \]

that is to say: 1 to 3, or 0.3333333.

\textbf{todo: redo the example of how you would find these odds in the
first place} An \emph{odds ratio} is a ratio of two odds, and it's a
useful way of comparing the odds for two different groups or scenarios.
Let's say if you want to know whether men or women are more likely to
purchase some product. Let's say the odds of a woman purchasing that
product are 1 to 4, or 0.25 (note, this is still and \emph{odds}, not a
probability-- it's just being expressed as a decimal instead of a
fraction). And let's say the odds of a man purchasing it are 1 to 3, or
0.33333. The \emph{odds ratio} of a man purchasing that product compared
to a woman is :

\[ OR_{male} = \frac{male \ odds}{female \ odds} \frac{1/3}{1/4} = \frac{0.3333}{0.25} \approx 1.3333 \]

What the odds ratio means is that a man is 1.3333 times as likely to
purchase that product as a woman is-- or, percentage-wise, that a man is
33.33\% more likely to purchase it.

If we want to know the odds ratio of a \emph{woman} purchasing the
product instead, we just flip the equation:

\[ OR_{female} = \frac{female \ odds}{male \ odds} \frac{1/4}{1/3} = \frac{0.25}{0.3333} = 0.75 \]

We read this as, ``a woman is 0.75 times as likely to purchase the
product as a man is'', or ``a woman is 25\% less likely to purchase the
product than a man is''.

Notice that these two values are not exact mirrors of each other--when
you're working with percent changes like this, your reference group
matters\textbf{todo: flesh this out way more}. Odds ratios can't go
below 0, but can go all the way up to infinity on the positive side.

You can also calculate an odds ratio along a continuous variable, by
comparing odds at two different points. Let's think about life
expectancy. Say your probability of surviving past age 35 was only 80\%
in 1895, but rose up to 85\% in 1896. The odds would be:

\[ odds_{1895} = \frac{0.8}{1-0.8} = 4 \]
\[ odds_{1896} = \frac{0.85}{1-0.85} = 5.6667 \]

To get the odds ratio of those two years, you would do:

\[ OR = \frac{odds_{1896}}{odds_{1895}} = \frac{5.6667}{4} = 1.417 \]
That is: in 1896, you were 1.417 times as likely to live past age 35 as
you were in 1895, or: You were 41.7\% more likely to live past 35 in
1896 as you were in 1895.

One more note: You remember our old friend the natural log, ln()? Let's
say for some reason that someone takes the natural log of an odds or
odds ratio, and gives that to you instead of the actual odds. This is
called a ``log odds'' (or ``log odds ratio'', respectively), and it's
easy to recover the original value: just take the exponent!

So, if I gave you a log odds ratio of 0.3483, you could recover the
original odds ratio by simply doing:

\[ e^{0.3483} = 1.417 \]

This will become relevant later.

\subsection{Script for Logistic regression
intro/motivation}\label{script-for-logistic-regression-intromotivation}

Why have I spent all this time bringing back awful memories from your
high school precalc class? We'll pull it all together here.

Let's say you want to run a regression, but your outcome variable isn't
continuous like ``salary'' or ``tip''. Instead, it's \emph{binary}-- a
yes/no or 0/1 variable, like ``did this person survive?'' or ``did this
person get accepted to medical school?'' The problem here is that you
have to take the continuous variable of your predictor, and \emph{map
it} to a space between 0 and 1. Traditional linear regression doesn't
let you do this-- it takes all possible values of your predictor
variable and maps them to a space from negative infinity to infinity.

Luckily, there's a super convenient function we can use to map our
continuous predictor variable onto an output variable that has to be
between zero and one: the \emph{logistic function}:

\[  \frac{1}{1+e^{-t}}\] Which looks like this:

{[}insert logistic plot{]}

We can make this into a regression by replacing that t value with a
regression equation:

\[ F(x) = \frac{1}{1+ e^{-(\beta_0 + \beta_1x)}}  \]

Now, the variables we input into our regression will be mapped to a
value between 0 and 1. This value can be interpreted as the probability
of the event in question occurring (survival, admittance to med school,
etc). You can convert these to binary values with a cutoff: usually, we
say that if the probability is less that 0.5, we predict 0, and if it's
greater than 0.5, we predict 1. So if we run logistic regression on
whether or not a student gets admitted to med school based on their
undergrad GPA, and then we predict a probability of getting into med
school for a new student, we would predict that the student \emph{does}
get into med school if her regression prediction was above 0.5, and
predict that she doesn't get into med school if her regression
prediction was below 0.5.

That's all well and good for prediction, but what on earth does this
equation actually \emph{mean}? What do the beta values mean?

We can answer that question by rearranging the logistic regression
equation to isolate just the part with the betas:

\[ F(x) = \frac{1}{1+ e^{-(\beta_0 + \beta_1x)}}  \]
\[ F(x) (1+ e^{-(\beta_0 + \beta_1x)}) = 1  \]

\[ 1+ e^{-(\beta_0 + \beta_1x)} = \frac{1}{F(x)}  \]
\[ e^{-(\beta_0 + \beta_1x)} = \frac{1}{F(x)} -1 = \frac{1-F(x)}{F(x)}  \]
But remember that e\^{}-x is just 1/e\^{}x, so:

\[ \frac{1}{e^{(\beta_0 + \beta_1x)}} = \frac{1-F(x)}{F(x)}   \] And
doing some more switching:
\[ e^{(\beta_0 + \beta_1x)} = \frac{F(x)}{1-F(x)}   \] Let's stop right
here, because this should look familiar. Remember that F(x) represents a
\emph{probability}. So the right hand side of this equation, F(x)
divided by 1-F(x), represents an \emph{odds}. So when we finally solve
for our beta equation:

\[ \beta_0 + \beta_1x = ln\left(\frac{F(x)}{1-F(x)} \right)  \]

Thats a \emph{log odds}. Our regression equation is describing the
\emph{log odds} of an event occurring. Our beta nought will itself be a
log odds, while all the other betas will be log odds \emph{ratios}.

\textbf{todo: flesh out all of these steps more}

The right hand side of this equation is called the logit function, by
the way. It's the inverse of the logistic function.

Let's use our example from before: We're trying to predict whether or
not a student will get into medical school, and we have two explanatory
variables: the gender of the student, and their GPA.

Let's look at gender first. If ``female'' is the baseline category, as
before, our beta-1 variable will represent \emph{males}. Let's say we
run this regression, and get an intercept of -1.609 and a beta 1 of
0.4054. Beta-0 is the \emph{log odds} of a female student getting into
med school. We can convert it back into an odds by taking the exponent:
e\^{}-1.609 = 0.2.

\[  e^{-1.609} = 0.2 \]

Beta-1 is the \emph{log odds RATIO} of a male student getting into med
school. If we exponentiate this value, we get :

\[  e^{0.4054} = 1.5 \] So, this regression is saying that males have
1.5 times the odds of getting into med school as females do. (note: this
is not true, I just made these numbers up as an example).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Now, let's imagine that we're using GPA as a predictor variable instead.
Let's say we get a beta-0 of -9.21, and a beta-1 of 1.163. As with our
linear regression, the intercept value corresponds to what we get when
our predictor variable equals zero. Here, our intercept value beta-0 is
the \emph{log odds} of getting into med school when your GPA is 0. We
should expect this to be \emph{extremely} unlikely:

\[ \beta_0 \ odds = e^{-9.21} = 0.0001 \] And indeed, our odds are
extremely low!

Our exponentiated beta-1 value tells us \emph{the odds ratio of a
one-point bump in GPA}. A whole point increase is huge in GPA terms--
that's the difference between a 2.0 and a 3.0. So, we should expect this
value to be pretty big.

\[ \beta_1 \ OR = e^{1.163} = 3.2 \] This says that a 1-unit increase in
GPA multiplys our odds of getting into medical school \emph{3.2 times} !
That's an increase in probability of 220\%!

Practically speaking (as with linear regression), we rarely think much
about the intercept value. Most of the focus is on betas greater than
zero)

I know there was a lot of math in this video, but if you remember
nothing else, remember this: \textbf{exponentiate your beta values to
get odds ratios. Except beta-0, exponentiate that to get an odds.
Interpretations of categorical/continuous variabls are analogous to
linear regression. }

\subsection{Script for example in R}\label{script-for-example-in-r}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Logistic regression example}
\KeywordTok{library}\NormalTok{(data.table)}
\KeywordTok{library}\NormalTok{(ggplot2)}

\KeywordTok{data}\NormalTok{(mtcars)}
\NormalTok{mtcars <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(mtcars)}

\CommentTok{# transform "am" to a categorical variable}
\NormalTok{mtcars[, type}\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(am}\OperatorTok{==}\DecValTok{1}\NormalTok{, }\StringTok{"manual"}\NormalTok{, }\StringTok{"auto"}\NormalTok{)]}

\CommentTok{# plot initial results}
\KeywordTok{ggplot}\NormalTok{(mtcars, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{mpg, }\DataTypeTok{y=}\NormalTok{vs)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size=}\DecValTok{2}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.7}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{expand_limits}\NormalTok{(}\DataTypeTok{y=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.25}\NormalTok{, }\FloatTok{1.25}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title=}\StringTok{"Getting Into UW Madison, vs ACT Score"}\NormalTok{,}
       \DataTypeTok{x=}\StringTok{"ACT Score"}\NormalTok{,}
       \DataTypeTok{y=}\StringTok{"Accepted to UW Madison?"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{logistic_regression_files/figure-latex/unnamed-chunk-1-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# logistic regression with mpg}
\NormalTok{logistic_mpg <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(vs}\OperatorTok{~}\NormalTok{mpg, }\DataTypeTok{data=}\NormalTok{mtcars, }\DataTypeTok{family=}\NormalTok{binomial)}
\KeywordTok{summary}\NormalTok{(logistic_mpg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = vs ~ mpg, family = binomial, data = mtcars)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2127  -0.5121  -0.2276   0.6402   1.6980  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)   
## (Intercept)  -8.8331     3.1623  -2.793  0.00522 **
## mpg           0.4304     0.1584   2.717  0.00659 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 43.860  on 31  degrees of freedom
## Residual deviance: 25.533  on 30  degrees of freedom
## AIC: 29.533
## 
## Number of Fisher Scoring iterations: 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot results }
\NormalTok{mtcars[, prob_mpg}\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{predict}\NormalTok{(logistic_mpg, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)]}
\KeywordTok{ggplot}\NormalTok{(mtcars, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{mpg)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y=}\NormalTok{vs), }\DataTypeTok{size=}\DecValTok{2}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.7}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{expand_limits}\NormalTok{(}\DataTypeTok{y=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.25}\NormalTok{, }\FloatTok{1.25}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y=}\NormalTok{prob_mpg), }\DataTypeTok{size=}\DecValTok{1}\NormalTok{, }\DataTypeTok{color=}\StringTok{"blue"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title=}\StringTok{"Getting Into UW Madison, vs ACT Score: Regression Results"}\NormalTok{,}
       \DataTypeTok{x=}\StringTok{"ACT Score"}\NormalTok{,}
       \DataTypeTok{y=}\StringTok{"Accepted to UW Madison?"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{logistic_regression_files/figure-latex/unnamed-chunk-1-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# logicstic regression with mpg and type}
\NormalTok{logistic_mpg_type <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(vs}\OperatorTok{~}\NormalTok{mpg}\OperatorTok{+}\NormalTok{type, }\DataTypeTok{data=}\NormalTok{mtcars, }\DataTypeTok{family=}\NormalTok{binomial)}
\KeywordTok{summary}\NormalTok{(logistic_mpg_type)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = vs ~ mpg + type, family = binomial, data = mtcars)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.05888  -0.44544  -0.08765   0.33335   1.68405  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)   
## (Intercept) -12.7051     4.6252  -2.747  0.00602 **
## mpg           0.6809     0.2524   2.698  0.00697 **
## typemanual   -3.0073     1.5995  -1.880  0.06009 . 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 43.860  on 31  degrees of freedom
## Residual deviance: 20.646  on 29  degrees of freedom
## AIC: 26.646
## 
## Number of Fisher Scoring iterations: 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# predict probability of a straight engine}
\NormalTok{mtcars[, prob_sengine}\OperatorTok{:}\ErrorTok{=}\KeywordTok{predict}\NormalTok{(logistic_mpg_type, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)]}

\CommentTok{#plot logistic results}
\KeywordTok{ggplot}\NormalTok{(mtcars, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{mpg)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y=}\NormalTok{vs, }\DataTypeTok{color=}\NormalTok{type)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y=}\NormalTok{prob_sengine, }\DataTypeTok{color=}\NormalTok{type), }\DataTypeTok{size=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{logistic_regression_files/figure-latex/unnamed-chunk-1-3.pdf}


\end{document}
